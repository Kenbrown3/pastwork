{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import and install necessary modules\n",
    "\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"tm\")\n",
    "install.packages(\"SnowballC\")\n",
    "install.packages(\"wordcloud\")\n",
    "install.packages(\"RColorBrewer\")\n",
    "install.packages(\"topicmodels\")\n",
    "install.packages(\"ldatuning\")\n",
    "install.packages(\"SentimentAnalysis\")\n",
    "\n",
    "library(tidytext)\n",
    "library(tm)\n",
    "library(SnowballC)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "library(topicmodels)\n",
    "library(ldatuning)\n",
    "library(SentimentAnalysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by installing the necessary modules for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data\n",
    "\n",
    "reviews <- read.csv(\"yelp.csv\", stringsAsFactors = F)         #Read calls from subset of first dataset                                                                          in which all calls entirely composed of                                                                          stopwords have been removed\n",
    "reviews <- as.matrix(calls[5])                                #Convert data to matrix\n",
    "reviews.vec <- VectorSource(calls)                            #Create data vector\n",
    "reviews.corpus <- Corpus(calls.vec)                           #Create corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read in our data and convert it into a corpus so that we can apply text mining methods to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Data\n",
    "\n",
    "reviews.corpus <- tm_map(reviews.corpus, tolower)                               #Remove capital letters\n",
    "reviews.corpus <- tm_map(reviews.corpus, removePunctuation)                     #Remove punctuation\n",
    "reviews.corpus <- tm_map(reviews.corpus, removeNumbers)                         #Remove numbers\n",
    "reviews.corpus <- tm_map(reviews.corpus, removeWords, stopwords(\"english\"))     #Remove stopwords\n",
    "reviews.corpus <- tm_map(reviews.corpus, stemDocument)                          #Stem words\n",
    "reviews.corpus <- tm_map(reviews.corpus, stripWhitespace)                       #Remove whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been read, it is necessary to clean it. Preprocessing makes the text more uniform and ultimately improves results. Here, we convert all text to lower case, remove punctuation, numbers, and stopwords, stem words, and remove extra whitespace. The benefits to each process are:\n",
    "\n",
    "**Lower case:** Converting all text to lower case ensures that words with different capitalizations are not counted differently. For example, we do not want \"Text\" and \"text\" to be counted as different words as this can significantly alter results. Perhaps this process is not desired if one wishes to understand what people start their sentences with, but that is not the aim of our project.\n",
    "\n",
    "**Remove Punctuation:** Again, this further renders our data uniform. It is also possible that failing to remove punctuation could count \"text-mining,\" \"text mining\", and \"text mining.\" differently.\n",
    "\n",
    "**Remove Numbers:** Although extracting numbers in a review may uncover certain information, they are difficult to work with and would most likely have very little impact on our project.\n",
    "\n",
    "**Remove Stop Words:** Stop words are words with very little predictive value. These often clutter the dataset without providing any insight. An added benefit is that working with the dataset after removing these words is more computationally efficient.\n",
    "\n",
    "**Stem Words:** Words of various conjugations and forms have the same meaning for our purposes so we do not want them to be counted separately. For example, \"visit,\" \"visited,\" and \"visitor\" all convey the same general meaning and thus we would like them grouped together. This method may backfire for certain words, but it generally improves results overall.\n",
    "\n",
    "**Stripping White Space:** After performing these pre-processing operations, we are left with many empty spaces which make the data difficult to read. Stripping unnecessary whitespace remedies this issue and makes our data more coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Frequencies\n",
    "\n",
    "#Create Term Document Matrix\n",
    "tdm <- TermDocumentMatrix(calls.corpus, \n",
    "        control=list(weighting=weightTfIdf))            #Specify weighting (currently TF-IDF)\n",
    "\n",
    "removeSparseTerms(tdm, 0.99)                           #Remove infrequent terms\n",
    "\n",
    "tdm <- as.matrix(tdm)                                  #Convert term document matrix to matrix form\n",
    "\n",
    "\n",
    "v <- sort(rowSums(tdm),decreasing=TRUE)                #Sort in order\n",
    "d <- data.frame(word = names(v),freq=v)                #Convert to dataframe\n",
    "head(d, 20)                                            #Preview data\n",
    "inspect(TermDocumentMatrix(calls.corpus,               #Inspect term document matrix\n",
    "        control = list(weighting = weightTfIdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is ready for analysis, we create a term-document matrix which shows the TF-IDF weight of each word in each review. Here, we remove infrequent terms to keep our matrix small while preserving its statistical abilities. Before moving on, we also sort our data, convert it into a dataframe, and inspect our term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Word Cloud\n",
    "\n",
    "set.seed(9162)\n",
    "wordcloud(words=d$word, freq=d$freq, min.freq=1,max.words=35, \n",
    "          random.order=FALSE, rot.per=0.3, colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word clouds are an effective exploratory methods in text-mining as they visually show important words in the corpus. As a result, we receive a quick, general idea of the kinds of words our dataset contains. More important (e.g. prevalent, high TF-IDF weight) words are larger and closer to the center of the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic Modeling Preparation\n",
    "\n",
    "dtm <- DocumentTermMatrix(review.corpus, control=list(weighting = weightTf)) #(Different than preivous tdm)\n",
    "rowTotals <- apply(dtm, 1, sum)\n",
    "dtm.new   <- dtm[rowTotals > 0, ]     #Modify new term document matrix for LDA\n",
    "\n",
    "#Finding ideal number of topics (currently evaluating between 5-15)\n",
    "#Takes a long time to run\n",
    "\n",
    "idealk <- FindTopicsNumber(dtm.new, topics = seq(from=5, to=15, by=1),\n",
    "  metrics = c(\"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n",
    "  method = \"VEM\", control = list(seed=77), verbose = TRUE)\n",
    "\n",
    "FindTopicsNumber_plot(idealk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we performed our topic modeling in Python, we wanted to see if we could find an optimal number of topics, k. This function uses various metrics to find the answer; however, we did not use these results to proceed with our analysis for various reasons. \n",
    "\n",
    "**1:** This method indicates our number of topics should be much higher than 15 (the maximum we tested), but we opted to use 10 because using many topics was computationally expensive\n",
    "\n",
    "**2:** This function is designed for finding the best number of LDA topics. We used both LDA and NMF in our analysis.\n",
    "\n",
    "**3:** This method takes a long time to run. We are currently running it between 5 and 15 topics and the marginal increase in running time increases with each additional topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "sent <- analyzeSentiment(calls.corpus)     #Sentiment Analysis\n",
    "plotSentiment(sent)                        #Plot\n",
    "\n",
    "binSent <- convertToBinaryResponse(sent)   #Binary response\n",
    "dirSent <- convertToDirection(sent)        #Directional response\n",
    "\n",
    "#Create sentiment datasets\n",
    "sentByMethod <- cbind(binSent[,1], binSent[,2], binSent[,5], binSent[8], binSent[,12])\n",
    "sentMethods2 <- cbind(dirSent[,1], dirSent[,2], dirSent[,5], dirSent[8], dirSent[,12])\n",
    "\n",
    "#Write sentiment datasets to CSV files\n",
    "write.csv(sentByMethod, file=\"Binary Review Sentiments.csv\")\n",
    "write.csv(sentMethods2, file=\"Directional Review Sentiments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we used sentiment analysis to understand how Yelp reviewers felt in their experiences. A quick look at the dataset shows that most reviews were positive, but we wanted a more in-depth understanding as well.\n",
    "\n",
    "The first thing we did was plot the sentiment of each review. This was not extremely useful for our purposes but could have been for identifying trends over time if our reviews came with a time-stamp.\n",
    "\n",
    "We then studied the binary and directional sentiments of each review through four different sentiment analysis dictionary. Each dictionary defines different words with a certain positivity or negativity scores, which the model aggregates. Using a binary response gives a score of \"positive\" or \"negative\" for each review and a directional response gives a score of \"positive,\" \"neutral,\" or \"negative.\"\n",
    "\n",
    "We then output the results of our analysis in CSV files. Each file contains the review number, the number of words in the review which were referenced by all dictionaries, and the label each dictionary provided the review. As a result we are left with four scores for each review. In Excel, we assigned values as such: positive = 1, neutral = 0, negative = -1 and averaged these scores to arrive at an ultimate \"average sentiment.\"\n",
    "\n",
    "Again, we saw that reviews are mostly positive, but this look provides a more concrete conclusion that simply using star ratings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
